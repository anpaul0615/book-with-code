# 3장 신경망 시작하기


## 3.1. 신경망의 구조

### 층 : 딥러닝의 구성 단위

- 하나 이상의 텐서를 입력으로 받아, 하나 이상의 텐서를 출력하는, "데이터 처리 모듈".

- 가중치..? => 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서.

- 층 내부에서는 네트워크가 학습한 지식(가중치) 을 가지고있음.


### 모델 : 층의 네트워크

- 층으로 구성된 비순환유향그래프(Directed Acyclic Graph, DAG).
  쉽게말해서 direct-graph + not-self-route 구조.

- 머신러닝..? => "가능성 있는 공간을 사전에 정의하고, 피드백 신호의 도움을 받아 입력데이터에 대한 유용한 표현을 찾는것".

- 가능성 있는 공간을 사전에 정의하고 => 입력데이터에서 출력데이터로 매핑하는 일련의 특정 텐서 연산(층) 을 정의하는것.  
  피드백 신호의 도움을 받아 입력데이터에 대한 유용한 표현을 찾는것 => 가중치 텐서의 좋은 값을 찾는것.  


### 손실함수와 옵티마이저 : 학습과정을 조절하는 열쇠

- 손실함수..? => 훈련하는 동안 최소화되어야하는 값.  
  옵티마이저..? => 손실함수를 기반으로 네트워크가 어떻게 업데이트될지 결정하는 주체.

- 여러개의 출력을 가지는 신경망은 여러개의 손실함수를 가질수있음.  
  경사하강법을 적용하기 위해 여러개의 손실값은 (평균을 내서) 하나의 스칼라 값으로 합쳐짐.

- 문제에 맞는 올바른 목적함수(손실함수) 를 선택해야함.  
  네트워크가 손실을 최소화하기위해 편법을 사용할수 있기 때문.  
  ex) 모든 인류의 평균행복지수를 최대화 하는 목적함수 => 몇사람을 남기고 모든 인류를 죽여서 남은 사람들의 행복에 초점을 맞춤

- 신경망은 단지 손실함수를 최소화하기만할뿐임.

- 올바른 손실함수를 선택하는 지침 :  
  2개 클래스 분류 문제 : 이진 크로스엔트로피(binary crossentropy)  
  n개 클래스 분류 문제 : 범주형 크로스엔트로피(categorical crossentropy)  
  회귀 문제 : 평균 제곱 오차  
  시퀀스 학습 문제 : CTC(Connection Temporal Classification)



## 3.2. 케라스 소개

## 3.3. 딥러닝 컴퓨터 세팅

## 3.4. 영화 리뷰 분류 : 이진 분류 문제

- [코드 실습 : 영화 리뷰 분류](./classifying-movie-reviews.ipynb)

- IMDB 데이터세트 :  
  훈련데이터 25000개, 테스트데이터 25000개  
  각각 50% 는 부정, 50% 는 긍정

- 데이터세트를 훈련데이터와 테스트데이터로 나누는 이유..?  
  => 같은 데이터에서 머신러닝 모델을 훈련하는 것은 금기사항!  
  모델이 훈련데이터에서 잘 작동한다고해서 새로운 데이터에서도 잘 작동하는것이 아니기때문.  

- 모델이 훈련샘플과 타깃 사이의 레이블 매핑을 모두 외워버릴수도 있음.  
  이런 모델은 처음 만나는 데이터에서 타깃을 예측하는 작업을 할수가 없음.

- 훈련데이터를 외우는 것이 전부인 학습알고리즘을 사례기반(instance-based) 학습이라고 하며, 대표적인 알고리즘으로는 k-최근접 이웃(k-Nearest Neighbor, KNN) 이 있음.  
  새로운 데이터에 대해 예측을 할때, 가장 가까운 훈련 데이터 몇개의 타깃을 평균(회귀) 하거나 다수인 클래스를 선택(분류) 하는방식으로 동작함.

- 신경망에 숫자리스트를 그대로 주입할수는 없음.  
  => 리스트를 텐서 형태로 바꿔줘야함.

- 리스트를 텐서로 바꾸는 두가지 방법 :  
  1) 각 리스트들이 같은 길이가 되도록 리스트에 패딩을 추가하고 (samples, sequence_length) 크기의 정수 텐서로 변환함  
  2) 리스트를 원-핫 인코딩하여 0과 1의 벡터로 변환함  

- `Dense(16, activation='relu')` 에서 Dense 층에 전달한 매개변수 16 은 은닉유닛(hidden unit) 의 개수임.  
  하나의 은닉유닛은 층이 나타내는 표현 공간에서 하나의 차원이 됨.
  
- `output = relu(dot(W, input) + b)` 에서 16개의 은닉유닛이 있다는 것은 가중치 행렬 W 의 크기가 (input_dimension, 16) 이라는 의미임.  
  즉 입력데이터와 W 를 점곱하면 입력데이터가 16차원으로 표현된 공간으로 투영됨. (그리고 b 를 더하고 relu 연산을 적용함)  
  쉽게말하면, 이 표현된 공간의 차원(=은닉유닛의 차원) 을 "신경망이 표현을 학습할때 가질수있는 자유도" 정도로 이해할수 있음.

- 은닉유닛을 늘리면(= 표현공간을 고차원으로 만들면) 신경망이 더욱 복잡한 표현을 학습할수있음.   
  하지만 계산비용이 커지고 원하지않는 패턴을 학습할수도 있음. (훈련데이터에서는 잘동작하지만, 테스트데이터에서는 잘동작하지않는 패턴)  

- 모델 중간에 있는 은닉층은 활성화함수로 relu 를 사용하고, 마지막층은 활성화함수로 sigmoid 를 사용함.  
  relu 함수 : 음수를 0 으로 만드는 함수 (입력데이터 처리 용도로 사용)  
  sigmoid 함수 : 임의의 값을 [0,1] 사이 값으로 표현하는 함수 (확률 출력 용도로 사용)  

- 손실함수는 binary_crossentropy 을 사용하고, 옵티마이저는 rmsprop 을 사용함.  
  binary_crossentropy : 확률 분포 간의 차이를 측정함  
  rmsprop : 과거의 모든 기울기를 균일하게 더하지 않고 새로운 기울기의 정보만 반영하도록 해서 학습률이 크게 떨어져지는 현상(0 에 가까워지는 현상) 을 방지함

- 과대적합(overfitting)..? => 훈련데이터에서는 잘 작동하는 모델이 처음보는 데이터에서는 잘 작동하지 않는 상황

- 훈련데이터와 검증데이터의 손실값과 정확도가 차이가 나타나는 에포크 지점을 특정하고, 그 에포크 이후 훈련을 중지하면 과대적합을 완화할수있음.  
  (이밖에 나머지 다른 과대적합 완화 방법들은 4장에서 확인)



## 3.5. 뉴스 기사 분류 : 다중 분류 문제

- [코드 실습 : 뉴스 기사 분류](./classifying-newswires.ipynb)

- 로이터 뉴스 데이터세트 :  
  훈련데이터 8982개, 테스트데이터 2246개 (뉴스기사 내용 단어사전인덱스 벡터)  
  각 데이터포인트에 대한 레이블 46개 (뉴스기사 토픽 인덱스 값)

- 각 데이터포인트가 정확히 하나의 범주로 분류됨. => 단일레이블 다중분류 문제  
  만약 각 데이터포인트가 여러개의 범주(ex. 토픽) 에 속할수있으면..? => 다중레이블 다중분류 문제

- 벡터라이징과 원핫인코딩의 차이..?  
  둘다 입력데이터를 신경망에서 처리할수있는 자료구조로 변환하는 방법임

- 벡터라이징은 데이터 샘플을 각 데이터포인터의 차원수를 최대차원수로 통일해서 규격화하는 방법임. (ex. 단어사전인덱스 벡터)  
  각 샘플마다 제각각인 차원수를 최대차원으로 통일하는것으로 신경망에서 연산가능한 텐서로 구성하는것이 주 목적임.

- 원핫인코딩은 데이터 레이블을 토픽의 인덱스를 의미하는 단일 값 하나에서 해당 토픽의 인덱스만 1 로 표현된 zero 벡터로 변환하는 방법임.  
  예를들어 단일 값 3 을 [ 0,0,3,0,0, .. ,0 ] 와 같은 일종의 on/off 벡터로 표현할수있음.  
  (※ 레이블뿐만아니라 데이터 샘플도 동일한 방식으로 원핫인코딩할수있음)

- 정보의 병목 현상..?  
  한 층에서 분류 문제에 필요한 일부 정보를 누락했을때, 그다음 층에서는 이를 복원할 방법이 없으므로 훈련에 필요한 정보를 완전히 잃게되는 현상.  
  이런 방식의 손실의 원인 대부분은 많은 정보를 중간층의 저차원(=정보 누락 차원) 으로 전달하기위해(=저차원에 맞추기위해) 압축하려고 했기 때문임.  
  네트워크에 충분히 큰 중간층을 구성해야되는 이유! 



## 3.6. 주택 가격 예측 : 회귀 문제

- [코드 실습 : 주택 가격 예측](./predicting-house-prices.ipynb)



## 3.7. 요약
