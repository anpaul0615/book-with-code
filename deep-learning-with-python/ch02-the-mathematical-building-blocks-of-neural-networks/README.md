# 2장 신경망의 수학적 구성 요소


## 2.1. 신경망과의 첫 만남

- [코드 실습 : MNIST 손글씨 숫자 이미지 분류](./mnist-demo.ipynb)

- 머신러닝에서 분류 문제의 범주(category) 를 클래스(class) 라고 함.  
  데이터 포인트는 샘플(sample) 이라고 함.  
  특정 샘플에 대한 클래스를 레이블(label) 이라고 함.

- 신경망의 핵심 구성 요소는 일종의 데이터 처리 필터라고 생각할 수 있는 층(layer) 이며,  
  이 층은 입력데이터로부터 의미있는 표현(representation) 을 추출함.

- 대부분의 딥러닝은 여러 층들이 연결된 구조로 되어 있고, 점진적으로 데이터를 정제하는 형태를 띄고있음.  
  즉 딥러닝 모델은 데이터 정제 필터인 층이 연속되어있는 "데이터 프로세싱을 위한 여과기" 라고 볼수있음.

- 완전 연결된(fully connected) 신경망 층인 Dense 층..?  
  이전 층의 출력을 입력으로 받는 층!

- 주로 신경망의 마지막에 위치해있는 소프트맥스(softmax) 층..?  
  손글씨 숫자 이미지를 최종적으로 분류하는 층!  
  마지막에 결론을 내는 층으로써 숫자 이미지 10개에 대한 확률 점수를 반환함,

- 신경망의 컴파일 단계에 포함되는 세가지 요소 :  
  손실 함수, 옵티마이저, 훈련과 테스트에 대한 평가 지표

- 손실 함수(loss function) :  
  훈련데이터에서 신경망의 성능을 측정하는 방법.  
  네트워크(모델) 가 올바른 방향으로 학습될 수 있도록 도와줌.

- 옵티마이저(optimizer) :  
  입력데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘임.

- 훈련과 테스트 과정을 모니터링할 지표 : 
  정확도(정확히 분류된 이미지의 비율) 등이 있음.

- 테스트 세트의 정확도가 훈련 세트 정확도보다 약간 낮게 나온 이유..?  
  과대적합 때문!

- 과대적합(overfitting) :  
  머신러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경항을 의미함.



## 2.2. 신경망을 위한 데이터 표현

- 텐서(tensor) 란..?  
  다차원 넘파이 배열  
  데이터를 위한 컨테이너(container)  
  임의의 차원 개수를 가지는 행렬의 일반화된 모습

- 텐서에서의 차원(dimension) == 텐서에서의 축(axis)

- 스칼라(0D 텐서) 란..?  
  하나의 숫자만 담고 있는 스칼라(scalar)  
  또는 스칼라 텐서, 0차원 텐서, 0D 텐서  
  스칼라 텐서의 축 개수는 0  

- 스칼라 텐서 예시
  ```python
  import numpy as np

  x = np.array(12)

  print(x)  ## array(12)
  print(x.ndim)  ## 0
  ```

- 텐서의 축 개수 == 랭크(rank)

- 벡터(1D 텐서) 란..?  
  숫자의 배열을(vector) 또는 1D 텐서  
  1D 텐서는 딱 하나의 축을 가짐

- 벡터 예시
  ```python
  x = np.array([12, 3, 6, 14, 7])

  print(x)  ## array([12, 3, 6, 14, 7])
  print(x.ndim)  ## 1
  ```

- 위 벡터는 5개의 원소를 가지고 있으므로 5차원 벡터라고 부름.  
  5D 벡터와 5D 텐서를 혼동하면 안됨.  
  5D 벡터는 하나의 축을따라 5개의 차원을 가진 것이고, 5D 텐서는 5개의 축을 가진 것임.

- 차원수(dimensionality) 란 특정 축을 따라 놓인 원소의 개수(ex. 5D 벡터) 이거나 텐서의 축 개수(ex. 5D 텐서) 를 의미함.

- 5D 텐서인 경우 랭크가 5 인 텐서라고 말하는것이 기술적으로는 좀 더 정확함. (텐서의 랭크 == 축의 개수)  
  그럼에도 5D 텐서 처럼 모호한 표기가 통용되고 있음.

- 행렬(2D 텐서) 이란..?  
  벡터의 배열을 행렬(matrix) 또는 2D 텐서 라고 부름.  
  행렬에는 2개의 축이 있음.  
  행렬은 숫자가 채워진사각 격자라고 생각하면 됨.

- 행렬 예시
  ```python
  x = np.array([[5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]])

  print(x.ndim)  ## 2
  ```

- 위 행렬에서 x 의 첫번째 행은 `[5,78,2,34,0]` 이고, 첫번째 열은 `[5,6,7]` 임.

- 3D 텐서와 고차원 텐서..?  
  행렬(2D 텐서) 들을 하나의 새로운 배열로 합친것을 3D 텐서 라고 함.  
  3D 텐서는 숫자가 채워진 직육면체 형태로 해석할 수 있음.  
  3D 텐서들을 하나의 배열로 합친것을 4D 텐서 라고 함.  

- 3D 텐서 예시
  ```python
  x = np.array([[[5, 78, 2, 34, 0],
                  [6, 79, 3, 35, 1],
                  [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                  [6, 79, 3, 35, 1],
                  [7, 80, 4, 36, 2]]
                  [[5, 78, 2, 34, 0],
                  [6, 79, 3, 35, 1],
                  [7, 80, 4, 36, 2]]])

  print(x.ndim)  ## 3
  ```

- 텐서의 핵심 속성 : 축의 개수, 크기, 데이터 타입

- 축의 개수(랭크) :  
  넘파이의 ndim 속성에 저장됨.  
  ex) 3D 텐서에는 3개의 축, 행렬에는 2개의 축  

- 크기(shape) :  
  파이썬 튜플(tuple) 로 표현됨.  
  텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타냄.  
  ex) 위 행렬예시에서의 크기는 (3,5)  
  ex) 위 3D 텐서 예시에서의 크기는 (3,3,5)  
  ex) 위 벡터예시에서의 크기는 (5,)  
  ex) 위 스칼라예시에서는 크기가 없음  

- 데이터 타입 :  
  넘파이의 dtype 속성에 저장됨.  
  텐서에 포함된 데이터의 타입.  
  float32, unit8, float64 등이 될 수 있음.  
  드물게 char 타입을 사용하기도하지만 보통 가변 문자열 타입은 지원하지 않음.  
  (텐서는 사전에 할당되어 연속된 메모리에 저장되어야하기 때문)  
  ex) 위 행렬예시에서는 float32 타입

- 넘파이 배열에서 특정 원소들을 선택하는것을 슬라이싱(slicing) 이라고 함.  
  ex) `train_images[10:100]`  
  ex) `train_images[10:100, :, :]`  
  ex) `train_images[10:100, :28, :28]`  
  ex) `train_images[:, :14, :14]`  
  ex) `train_images[:, 7:-7, 7:-7]`  

- 일반적으로 딥러닝에서 사용하는 데이터 텐서의 첫번째 축을 샘플 축(sample axis) 또는 샘플 차원(sample dimension) 이라고 함.  
  ex) 이미지 데이터 텐서 : (samples, height, width, color_depth)  
  ex) 동영상 데이터 텐서 : (samples, frames, height, width, color_depth)  

- 딥러닝 모델은 데이터를 작은 배치(batch) 단위로 나누어서 전체 데이터세트를 처리함.  
  예를들어, MNIST 숫자 데이터에서 크기가 128 인 배치는  `batch = train_images[:128]` 로 표현되고, 그다음 배치는 `batch = train_images[128:256]` 이며, n 번째 배치는 `batch = train_images[128*n : 128*(n+1)]` 로 표현됨.

- 이런 배치 데이터를 다룰때는 첫번째 축을 배치 축(batch axis) 또는 배치 차원(batch dimension) 이라고 부름.

- 텐서의 실제 사례 :  
  벡터데이터 => (samples, features)  
  시계열데이터 => (samples, timesteps, features)  
  이미지데이터 => (samples, height, width, color_depth)  
  동영상데이터 => (samples, frames, height, width, color_depth)  

- 벡터 데이터(2D 텐서) :  
  첫번째 축은 샘플 축이고, 두번째 축은 특성 축  
  ex) 나이/우편번호/소득 으로 구성된 인구 통계 데이터에서, 각 사람은 3개의 값을 가진 벡터로 구성되고, 10만명이 포함된 전체 데이터 세트는 `(100000, 3)` 크기의 2D 텐서로 인코딩됨.  
  ex) 공통 단어 2만개와 각 단어가 등장한 횟수로 표현된 텍스트 문서 데이터세트에서, 각 문서는 2만개의 원소를 가진 벡터로 인코딩되고, 500개의 문서로 이루어진 전체 데이터세트는 `(500, 20000)` 크기의 2D 텐서로 인코딩됨.

- 시계열 데이터(3D 텐서) :  
  데이터에서 시간이나 연속된 순서가 중요할때는 시간축을 포함하여 3D 텐서로 구성함.  
  각 샘플은 벡터(2D 텐서)의 시퀀스로 인코딩 되므로, 배치 데이터는 3D 텐서로 인코딩 됨.  
  관례적으로 시간축은 항상 두번째 축(인덱스가 1번인 축) 으로 구성됨.  
  ex) 현재주식가격/지난1분간최고가격/최소가격 으로 구성된 주식 가격 데이터에서, 1분마다 데이터가 인코딩된다고 할때, 하루 거래시간 09:30~16:00 동안의 총 거래데이터는 `(390, 3)` 크기(형태)의 2D 텐서로 인코딩되고, 이 데이터의 250일치의 데이터는 `(250, 390, 3)` 크기의 3D 텐서로 저장될수있음.  
  ex) 128개의 알파벳과 280자의 문자시퀀스로 구성된 트윗 데이터에서, 각 문자는 128 크기인 이진벡터로 표현할수있고(원-핫인코딩), 이 문자데이터의 280자 조합으로 이루어진 트윗 데이터는 `(280, 128)` 크기의 2D 텐서로 인코딩될수있고, 100만개의 트윗으로 구성된 데이터세트는 `(1000000, 280, 128)` 크기의 텐서로 저장될수있음.

- 이미지 데이터(4D 텐서) :  
  이미지는 전형적으로 높이/너비/컬러 채널의 3차원으로 이루어짐.  
  흑백이미지는 하나의 컬러채널만을 가지고있어 2D 텐서로 저장될수도있지만, 관례상 이미지 텐서는 항상 3D 로 저장함. (흑백이미지인경우 컬러채널의 차원크기는 1, 그레이스케일)  
  ex) 256*256 크기의 흑백이미지에 대한 128개의 배치는 `(128, 256, 256, 1)` 크기의 텐서에 저장될수있음.  
  ex) 컬러이미지에 대한 128개의 배치는 `(128, 256, 256, 3)` 크기의 텐서에 저장될수있음.

- 동영상 데이터(5D 텐서) :  
  하나의 비디오는 프레임의 연속이고, 각 프레임은 하나의 컬러이미지와 동일함.  
  프레임은 `(height, width, color_depth)` 의 3D 텐서로 저장될수있으므로, 프레임의 연속은 `(frames, height, width, color_depth)` 의 4D 텐서로 저장될수있음.  
  여러 비디오의 매치는 `(samples, frames, height, width, color_depth)` 의 5D 텐서로 저장될수있음.  
  ex) 60초짜리 144*256 비디오클립을 초당 4프레임으로 샘플링한 240프레임/분 으로 된 비디오 데이터에서, 이 비디오클립을 4개 가진 배치는 `(4, 240, 144, 256, 3)` 크기의 5D 텐서에 저장됨.



## 2.3. 신경망의 톱니바퀴 : 텐서 연산

- 텐서 연산(tensor operation) :  
  신경망의 입력 또는 출력으로 사용되는 표현인 텐서에 대한 변환 작업  
  심층 신경망이 학습한 모든 변환을 수치 데이터 텐서에 적용하는데 사용됨.  
  ex) 텐서 덧셈, 텐서 곱셈, 점곱  

- 텐서 연산 구성 :  
  `output = relu( dot(W, input) + b)` 에서,  
  relu(x) 는 max(x, 0) 를 수행하는 함수  
  dot(a,b) 은 a 와 b 사이의 점곱  
  W 는 가중치, b 는 바이어스

- relu 함수는 원소별 연산(element-wise, operation) 으로 구분됨.  
  원소별 연산은 텐서에 있는 각 원소에 독립적으로 적용됨. (입력에 대한 원본을 유지)

- 브로드캐스팅(broadcasting) :  
  크기가 다른 두 텐서에 대해서, 작은 텐서가 큰 텐서의 크기에 맞추는 현상.

- 브로드캐스팅 동작 방식 :  
  1) 큰 텐서의 ndim 에 맞도록 작은 텐서에 브로드캐스팅 축이 추가됨.  
  2) 작은 텐서가 새 축을 따라서 큰 텐서의 크기만큼 반복해서 복사됨.  

- 브로트캐스팅 동작 방식 예시 :  
  (32,10) 텐서와 (10,) 텐서 사이에서 브로드캐스팅이 적용되면, 작은 텐서에 새 축이 하나 추가되고, 이 축을 따라서 작은 텐서를 큰 텐서의 첫번째 축 차원수만큼 작은텐서를 복사하면, (10,) 이 32 번 반복된 (32,10) 텐서가 만들어지게 됨.

- 텐서의 점곱(tensor produdct) :  
  입력 텐서의 원소들을 결함시킴.  
  점곱 연산 대상 원소들의 곱셈 결과값을 누적함. (새로운 스칼라)  
  x dot y 에서 x의 행의 크기와 y 의 열의 크기가 같아야함.  
  ex) 벡터와 벡터의 점곱 => 새로운 스칼라  
  ex) 행렬과 벡터의 점곱 => 새로운 벡터  
  ex) (a,b,c,d) dot (d,) => (a,b,c)  
  ex) (a,b,c,d) dot (d,e) => (a,b,c,e)  

- 텐서 크기 변환(tensor reshaping) :  
  주로 신경망에 주입할 숫자 데이터를 전처리할때 사용.  
  텐서의 크기를 변환한다는 것은 특정 크기에 맞게 열과 행을 재배열한다는 뜻임.  
  변환된 텐서는 원래의 텐서와 원소 개수가 동일함.  
  ```python
  x = np.array( [[0., 1.],
                [2., 3.],
                [4., 5.]] )
  print(x.shape)  ## (3,2)

  x = x.reshape( (6,1) )
  print(x)
  ## [[0.],
  ##  [1.],
  ##  [2.],
  ##  [3.],
  ##  [4.],
  ##  [5.]]

  x = x.reshape( (2,3) )
  print(x)
  ## [[ 0., 1., 2. ],
  ##  [ 3., 4., 5. ]]
  ```

- 자주 사용되는 크기 변환은 전치(transposition) 연산임.  
  행렬의 전치는 행과 열을 바꾸는 것을 의미함.  
  예를들어 x[i, :] 에서 x[:, i] 으로 변경되는 경우.  
  ```python
  x = np.zeros( (300,20) )
  print(x.shape)  ## (300,20)
  x = np.transpose(x)
  print(x.shape)  ## (20,300)
  ```

- 모든 텐서 연산은 기하학적 해석이 가능함.  
  예를들어 두 벡터를 더해서 나온 새로운 벡터를 좌표평면으로 표현.  
  텐서의 일반적인 기하학적 표현으로는 아핀변환, 회전, 스케일링 등이 있음.

- 신경망은 전체적으로 텐서 연산의 연결로 구성된 것이고, 모든 텐서 연산은 입력데이터의 기하학적 변환임.  

- 딥러닝의 기하학적 해석 예시 :  
  1) 빨간색 파란색 색종이 두장을 겹치고 뭉쳐서 하나의 작은 종이공으로 만들었을때, 이 종이공이 입력데이터고 색종이는 분류 클래스 임  
  2) 신경망이 해야할일은 이 종이공을 펼쳐서 두개의 분류 클래스로 깔끔하게 분리되는 변환을 찾는 것임  

- 위의 예시처럼, 종이공을 펼치는 것이 머신러닝이 하는일임.  
  즉, 복잡하고 심하게 꼬여있는 데이터의 매니폴드(다양체)에 대해서 깔끔한 표현을 찾는일임.

- 딥러닝은 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식을 사용하는데, 이것이 마치 사람이 종이공을 펼치는 과정과 매우 흡사함.  
  심층 네트워크의 각 층은 데이터를 조금씩 풀어주는 변환을 수행하므로, 이런 층을 깊게 쌓으면 아주 복잡한 분해과정을 처리할수있음.  



## 2.4. 신경망의 엔진 : 그래디언트 기반 최적화

## 2.5. 첫 번째 예제 다시 살펴보기

## 2.6. 요약

