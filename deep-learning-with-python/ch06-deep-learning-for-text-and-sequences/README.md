# 6장 텍스트와 시퀀스를 위한 딥러닝

- 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델 :  
  - 순환 신경망(recurrent neural network)  
  - 1D 컨브넷(1D convnet)  

- 애플리케이션 예시 : 
  - 문서 분류나 시계열 분류; 예를들어 글의 주제나 책의 저자 식별하기  
  - 시계열 비교; 예를들어 두 문서나 주 주식 가격이 얼마나 밀접하게 관련이 있는지 추정하기  
  - 시퀀스-투-시퀀스 학습; 예를들어 영어 문장을 프랑스어로 변환하기  
  - 감정 분석; 예를들어 트윗이나 영화리뷰가 긍정적인지 부정적인지 분류하기  
  - 시계열 예측; 예를들어 어떤 지역의 최근 날씨 데이터가 주어졌을때 향후 날씨 예측하기


## 6.1. 텍스트 데이터 다루기

- 텍스트는 가장 흔한 형태의 시퀀스 데이터임. (=단어의 시퀀스 또는 문자의 시퀀스)

- 컴퓨터 비전이 픽셀에 적용한 패턴인식인 것처럼, 자연어 처리를 위한 딥러닝은 단어/문장/문단에 적용한 패턴인식임.

- 다른 모든 신경망과 마찬가지로 텍스트 원본을 입력데이터로 사용할수 없음.  
  딥러닝은 수치형 텐서만 다룰수있음 => 텍스트 벡터화(vectorizing text)

- 텍스트 벡터화의 종류 :  
  - 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환함
  - 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환함
  - 텍스트에서 단어나 문자의 n-gram 을 추출하여 각 n-gram 을 하나의 벡터로 변환함  
    (※ n-gram 은 문자에서 추출한 n개(또는 그 이하) 의 연속된 단어그룹)

- 텍스트를 나누는 단위(단어/문자/n-gram)들을 토큰 이라고 함.  
  그리고 텍스트를 토큰으로 나누는 작업을 토큰화(tokenization) 라고 함.

- 모든 텍스트 벡터화 작업은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는것으로 이루어짐.  
  ex) 토큰에 대한 원-핫 인코딩(ont-hot encoding)  
  ex) 토큰 임베딩(token embedding) (=단어 임베딩, word embedding)


### 단어와 문자의 원-핫 인코딩

- 원-핫 인코딩 :  
  모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i 를 크기가 N(어휘사전의 크기) 인 이진벡터로 변환하는 방법  
  변환된 이진벡터는 i 번째 원소만 1 이고 나머지는 모두 0 인 벡터임

- 원-핫 해싱 :  
  원-핫 인코딩의 변종 기법으로, 어휘사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울때 사용하는 방법  
  각 단어에 명시적 인덱스를 할당하고, 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환함  
  명시적인 단어 인덱스가 필요없기때문에 메모리를 절약할수있고 온라인 방식으로 데이터를 인코딩할수있음  
  한가지 단점은 해시충돌인데, 이는 해싱공간의 차원을 해싱결과토큰의 전체 개수보다 훨씬 크게하면 어느정도 충돌을 피할수있음

- [코드 실습 : 단어와 문자의 원-핫 인코딩](./one-hot-encoding-of-words-or-characters.ipynb)


### 단어 임베딩 사용하기

- 단어 임베딩 :  
  밀집 단어 벡터를 사용하는 방법  
  원-핫 인코딩으로 만든 벡터는 대부분 0 으로 채워져있기때문에 희소(sparse) 하고, 어휘사전에 있는 단어의 수와 차원이 같기때문에 매우 고차원임  
  반면에 단어 임베딩은 언어를 기하학적 공간에 매핑하는 방식을 사용하므로 단어 표현이 조밀하고, 상대적으로 낮은 차원으로 구성됨  

- 단어 임베딩 특징 :  
  잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩 됨  
  일반적으로 두 단어 벡터 사이의 거리(=L2 거리) 는 두 단어 사이의 의미상 거리와 관련되어있음  
  즉 멀리떨어진 위치에 임베딩된 단어 의미는 서로 다른 반면 비슷한 단어들은 가까이에 임베딩 되는 방식임  
  거리 외에도 임베딩 공간의 특정 방향도 의미를 가질수 있음

- 단어 임베딩 예시 :  
  ex) cat → tiger 로 이동하는 것과 dog → wolf 로 이동하는 것을 같은 벡터로 나타낼수있음 (애완동물에서 야생동물로 이동)  
  ex) dog → cat 으로 이동하는 것과 wolf → tiger 로 이동하는것을 같은 벡터로 나타낼수있음 (개과에서 고양이과로 이동)  
  ex) king 벡터에 female 벡터를 더하면 queen 벡터가 되고, plural 벡터를 더하면 kings 벡터가 됨

- 이상적인 단어 임베딩 공간은 존재하지 않음.  
  새로운 작업에는 새로운 임베딩을 학습하는것이 타당함.

- 케라스의 Embedding Layer 를 사용하여 새로운 단어 임베딩을 구성할수있음.  
  Embedding 층을 특정 단어를 나타내는 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해할수있음.  
  정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환하는 방식으로 동작함.  
  `단어 인덱스 입력 => Embedding 층 탐색 => 연관된 단어 벡터 반환`

- 단어 임베딩을 직접 구성하는 대신에, 각 문제에 맞는 사전 훈련된 임베딩 공간을 사용할수도 있음.  
  자신만의 좋은 특성을 학습하기는 어렵지만 일반적인 특성에 대해서는 훌륭하게 동작함.  
  잘 알려전 단어 임베딩 방법으로는 Word2vec, GloVe 등이 있음.

- [코드 실습 : 단어 임베딩](./using-word-embeddings.ipynb)



## 6.2. 순환 신경망 이해하기

## 6.3. 순환 신경망의 고급 사용법

## 6.4. 컨브넷을 이용한 시퀀스 처리
