# 6장 텍스트와 시퀀스를 위한 딥러닝

- 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델 :  
  - 순환 신경망(recurrent neural network)  
  - 1D 컨브넷(1D convnet)  

- 애플리케이션 예시 : 
  - 문서 분류나 시계열 분류; 예를들어 글의 주제나 책의 저자 식별하기  
  - 시계열 비교; 예를들어 두 문서나 주 주식 가격이 얼마나 밀접하게 관련이 있는지 추정하기  
  - 시퀀스-투-시퀀스 학습; 예를들어 영어 문장을 프랑스어로 변환하기  
  - 감정 분석; 예를들어 트윗이나 영화리뷰가 긍정적인지 부정적인지 분류하기  
  - 시계열 예측; 예를들어 어떤 지역의 최근 날씨 데이터가 주어졌을때 향후 날씨 예측하기


## 6.1. 텍스트 데이터 다루기

- 텍스트는 가장 흔한 형태의 시퀀스 데이터임. (=단어의 시퀀스 또는 문자의 시퀀스)

- 컴퓨터 비전이 픽셀에 적용한 패턴인식인 것처럼, 자연어 처리를 위한 딥러닝은 단어/문장/문단에 적용한 패턴인식임.

- 다른 모든 신경망과 마찬가지로 텍스트 원본을 입력데이터로 사용할수 없음.  
  딥러닝은 수치형 텐서만 다룰수있음 => 텍스트 벡터화(vectorizing text)

- 텍스트 벡터화의 종류 :  
  - 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환함
  - 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환함
  - 텍스트에서 단어나 문자의 n-gram 을 추출하여 각 n-gram 을 하나의 벡터로 변환함  
    (※ n-gram 은 문자에서 추출한 n개(또는 그 이하) 의 연속된 단어그룹)

- 텍스트를 나누는 단위(단어/문자/n-gram)들을 토큰 이라고 함.  
  그리고 텍스트를 토큰으로 나누는 작업을 토큰화(tokenization) 라고 함.

- 모든 텍스트 벡터화 작업은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는것으로 이루어짐.  
  ex) 토큰에 대한 원-핫 인코딩(ont-hot encoding)  
  ex) 토큰 임베딩(token embedding) (=단어 임베딩, word embedding)


### 단어와 문자의 원-핫 인코딩

- 원-핫 인코딩 :  
  모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i 를 크기가 N(어휘사전의 크기) 인 이진벡터로 변환하는 방법  
  변환된 이진벡터는 i 번째 원소만 1 이고 나머지는 모두 0 인 벡터임

- 원-핫 해싱 :  
  원-핫 인코딩의 변종 기법으로, 어휘사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울때 사용하는 방법  
  각 단어에 명시적 인덱스를 할당하고, 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환함  
  명시적인 단어 인덱스가 필요없기때문에 메모리를 절약할수있고 온라인 방식으로 데이터를 인코딩할수있음  
  한가지 단점은 해시충돌인데, 이는 해싱공간의 차원을 해싱결과토큰의 전체 개수보다 훨씬 크게하면 어느정도 충돌을 피할수있음

- [코드 실습 : 단어와 문자의 원-핫 인코딩](./one-hot-encoding-of-words-or-characters.ipynb)


### 단어 임베딩 사용하기

- 단어 임베딩 :  
  밀집 단어 벡터를 사용하는 방법  
  원-핫 인코딩으로 만든 벡터는 대부분 0 으로 채워져있기때문에 희소(sparse) 하고, 어휘사전에 있는 단어의 수와 차원이 같기때문에 매우 고차원임  
  반면에 단어 임베딩은 언어를 기하학적 공간에 매핑하는 방식을 사용하므로 단어 표현이 조밀하고, 상대적으로 낮은 차원으로 구성됨  

- 단어 임베딩 특징 :  
  잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩 됨  
  일반적으로 두 단어 벡터 사이의 거리(=L2 거리) 는 두 단어 사이의 의미상 거리와 관련되어있음  
  즉 멀리떨어진 위치에 임베딩된 단어 의미는 서로 다른 반면 비슷한 단어들은 가까이에 임베딩 되는 방식임  
  거리 외에도 임베딩 공간의 특정 방향도 의미를 가질수 있음

- 단어 임베딩 예시 :  
  ex) cat → tiger 로 이동하는 것과 dog → wolf 로 이동하는 것을 같은 벡터로 나타낼수있음 (애완동물에서 야생동물로 이동)  
  ex) dog → cat 으로 이동하는 것과 wolf → tiger 로 이동하는것을 같은 벡터로 나타낼수있음 (개과에서 고양이과로 이동)  
  ex) king 벡터에 female 벡터를 더하면 queen 벡터가 되고, plural 벡터를 더하면 kings 벡터가 됨

- 이상적인 단어 임베딩 공간은 존재하지 않음.  
  새로운 작업에는 새로운 임베딩을 학습하는것이 타당함.

- 케라스의 Embedding Layer 를 사용하여 새로운 단어 임베딩을 구성할수있음.  
  Embedding 층을 특정 단어를 나타내는 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해할수있음.  
  정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환하는 방식으로 동작함.  
  `단어 인덱스 입력 => Embedding 층 탐색 => 연관된 단어 벡터 반환`

- 단어 임베딩을 직접 구성하는 대신에, 각 문제에 맞는 사전 훈련된 임베딩 공간을 사용할수도 있음.  
  자신만의 좋은 특성을 학습하기는 어렵지만 일반적인 특성에 대해서는 훌륭하게 동작함.  
  잘 알려전 단어 임베딩 방법으로는 Word2vec, GloVe 등이 있음.

- [코드 실습 : 단어 임베딩](./using-word-embeddings.ipynb)



## 6.2. 순환 신경망 이해하기

- 완전 연결 네트워크나 컨브넷처럼 지금까지 본 모든 신경망의 특징은 메모리가 없다는 것임.  
  즉 네트워크에 주입되는 모든 입력은 개별적으로 처리되며 입력 사이에 유지되는 상태가 없다는 것임.  
	이런 네트워크에서 시퀀스 데이터를 처리하려면, 네트워크에 전체 시퀀스를 주입해야함. (즉 전체 시퀀스를 하나의 데이터포인트로 변환)  
	이런 네트워크를 피드포워드 네트워크(feedforward network) 라고 함.

- 이와 반대로 이전에 나온 데이터 포인트들을 기억하면서 입력을 처리할수있는 신경망도 있음.  
  마치 생물학적 지능처럼, 정보처리를 위한 내부 모델을 유지하면서 점진적으로 정보를 처리해나갈수있음.  
  즉 과거 정보를 사용하여 모델을 구축하고 새롭게 얻은 정보를 사용하여 모델을 업데이트 해나가는 방식으로 동작함.  
  이런 네트워크들중 하나가 순환신경망(Recurrent Neural Network, RNN) 임.  

- RNN 은 시퀀스의 각 원소를 순회하면서 지금까지 처리한 정보를 상태에 저장하는 방식으로 동작함. (내부에 루프를 가진 신경망의 한 종류라고 볼수있음)  
  즉 네트워크에 하나의 입력을 주입한다했을때, RNN 은 각 시퀀스 원소를 차례대로 순회하면서 상태를 갱신하는 방식으로 동작함.  
  쉽게말하면 2개의 다른 시퀀스(ex. IMDB 에서의 2개의 다른 리뷰) 를 처리하는 과정에서 RNN 의 상태가 재설정되는 것임.

- 의사코드로 표현된 RNN  
  ```
  state_t = 0  // 최초 상태
  for input_t in input_sequence:  // 시퀀스 순회
      output_t = f(input_t, state_t)  // 입력+상태로 출력 계산
      state_t = output_t  // 다음 스텝에서 사용될 상태 갱신
  ```


### 케라스의 순환층

- SimpleRNN :   
  입력으로 (batch_size, timestemps, input_features) 을 받음  
  1) 타임스텝의 출력을 모은 전체 시퀀스를 반환하거나, 2) 입력 시퀀스에 대한 마지막 출력만 반환하는 방식으로 동작함


### LSTM 과 GRU 층 이해하기

- SImpleRNN 은 이론적으로 시간 t 에서 이전의 모든 타임스텝의 정보를 유지할 수 있음.  
  하지만 실제로는 긴 시간에 걸친 의존성은 학습할 수 없음. (그래디언트 소실 문제 때문)

- 그래디언트 소실 문제(vanishing gradient problem) :  
  순환신경망의 역전파는 타임스텝에 따라 네트워크가 펼쳐진것처럼 진행됨  
  역전파가 진행되는 동안 타임스텝마다 동일한 가중치를 사용하기 때문에, 타임스텝이 길어질수록 그래디언트값이 급격하게 줄어들거나 급격하게 증가할수있음

- LSTM :  
  SImpleRNN 의 변종으로, 정보를 여러 타임스텝에 걸쳐 나르는 방법이 적용된 방법임  
  시퀀스 어느 지점에서 추출된 정보가 필요한 시점의 타임스텝에서 사용되도록 구성됨  
  나중을 위해 정보를 저장함으로써 처리 과정에서 오래된 시그널이 점차 소실되는것을 막아줌

- 의사코드로 표현된 LSTM  
```
output_t = activation(c_t) * activation(dot(input_t, Wo)) + dot(state_t, Uo) + bo)

i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)  // sigmoid
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)  // sigmoid
k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)  // tanh
```

```
(c_t + 1) = (i_t * k_t) + (c_t * f_t)
// 다음트랙 상태 = 현재트랙 입력 선정 + 현재트랙 상태 선정
```

- 쉽게말하면, LSTM 셀에서 과저겅보를 다시 주입하는것으로 그래디언트 소실문제를 해결한다는 의미임.


### LSTM 층으로 IMDB 데이터세트 훈련

- SimpleRNN 네트워크보다 검증 정확도가 더 좋은 이유..?  
  => 그래디언트 소실문제로부터 영향을 덜 받기 때문!  
  => 하지만 많은 계산을 한것치고는 획기적인 결과는 아님

- LSTM 의 성능이 더 높지 않은 이유는..?  
  => 하이퍼파리미터 튜닝, 네트워크 규제  
  => 가장 큰 이유는 감성분류문제에서 시퀀스 모델을 사용했다는 것임  
  => 리뷰데이터(IMDB) 를 전체적으로 길게 분석하는 것은 감성 분류에 도움이 되지 않음  
  => 이런 문제는 각 리뷰에 어떤 단어가 나타나고 또 얼마나 등장하는지를 보는것이 더 나은 방법임



## 6.3. 순환 신경망의 고급 사용법

## 6.4. 컨브넷을 이용한 시퀀스 처리
